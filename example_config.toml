# Example .simfs_config.toml file
# Place this file in any directory within your simfs mount
# to configure the model used for generating files in that directory

# Model name - this should match the model names supported by your LLM endpoint
# For koboldcpp, common models include:
# - "meta-llama/Llama-3.2-3B-Instruct" (default)
# - "gpt-3.5-turbo"
# - "gpt-4"
# - Or any custom model name your koboldcpp instance supports
model = "meta-llama/Llama-3.2-3B-Instruct"

# Future configuration options (not yet implemented):
# temperature = 0.7
# max_tokens = 2048
# system_prompt = "You are a creative writer..."